{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### League of Legends: Model Training\n",
    "Standard Methodology:\n",
    "\n",
    "1. Exploratory plots to get a sense of data (e.g. relationships, distribution etc.)\n",
    "2. **Perform transformations (standardization, log-transform, PCA etc.)**\n",
    "3. **Experiment with algorithms that make sense, feature selection and compare cross-validated performance.Algos to thinks about: Tree-Based, Basis Expansion, Logistic Regression, Discriminant  Analysis, Boosting, Neural Nets...**\n",
    "\n",
    "4. **Run on test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import math\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "#BaseEstimator will inherit get_parms and set_parms methods. \n",
    "#TransformerMixin will inherit fit_transform, which calls fit and transform. We can customize our fit and transform\n",
    "#These are used for consistency with existing sklearn classes\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_pickle(path):\n",
    "    \n",
    "    input_file = open(path,'rb')\n",
    "    variable = pickle.load(input_file)\n",
    "    input_file.close()\n",
    "    return(variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = '../data/x_train.pickle'\n",
    "x_test = '../data/x_test.pickle'\n",
    "y_train = '../data/y_train.pickle'\n",
    "y_test = '../data/y_test.pickle'\n",
    "\n",
    "x_train = read_pickle(x_train) \n",
    "x_test = read_pickle(x_test) \n",
    "y_train = read_pickle(y_train) \n",
    "y_test = read_pickle(y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations ###\n",
    "\n",
    "Let's begin with applying the transformations we deemed suitable during EDA. \n",
    "1. Standardize the data, \n",
    "2. Remove crit and crit per level variables \n",
    "3. Log cs feature \n",
    "4. Create per level * gamelength variable\n",
    "5. Perform PCA with 30 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom transformer\n",
    "class FeatureEngineering(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    __slots__ = ['x_df']\n",
    "    \n",
    "    #Initiate class\n",
    "    def __init__(self, x_df, pca_components): \n",
    "        self.x_df = x_df\n",
    "        self.pca_components = pca_components\n",
    "        \n",
    "    #We don't need to fit anything, so leave this as is\n",
    "    def fit(self, x_df):\n",
    "        return self\n",
    "    \n",
    "    #Perform our feature transformations\n",
    "    def transform(self, x_df):\n",
    "        \n",
    "        #Standardize data\n",
    "        standard_scaler = preprocessing.StandardScaler()\n",
    "        x_scaled = standard_scaler.fit_transform(x_df)\n",
    "        x_df = pd.DataFrame(x_scaled, columns = x_df.set_index('gameid').columns)\n",
    "        \n",
    "        #Log cs field\n",
    "        x_df['log_delta_total_cs'] = math.log(x_df['delta_total_cs'])\n",
    "        x_df = x_df.drop('delta_total_cs', axis = 1)\n",
    "        \n",
    "        #Create per_level * gamelength variables\n",
    "        feature_columns = x_df.columns\n",
    "        per_level = [feature for feature in feature_columns if \"perlevel\" in feature]\n",
    "        \n",
    "        for i in per_level:\n",
    "            field_name = i + str('_gamelength')\n",
    "            x_df[field_name] = x_df[i] * x_df['gamelength']\n",
    "            \n",
    "        #30 component PCA\n",
    "        pca = PCA(n_components = self.pca_components) #subtracting out crit columns\n",
    "        x_df = pca.fit_transform(x_df.values)\n",
    "        \n",
    "        self.x_df = x_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
