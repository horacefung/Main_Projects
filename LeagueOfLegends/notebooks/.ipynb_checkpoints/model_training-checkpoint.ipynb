{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### League of Legends: Model Training\n",
    "Standard Methodology:\n",
    "\n",
    "1. Exploratory plots to get a sense of data (e.g. relationships, distribution etc.)\n",
    "2. **Perform transformations (standardization, log-transform, PCA etc.)**\n",
    "3. **Experiment with algorithms that make sense, feature selection and compare cross-validated performance.Algos to thinks about: Tree-Based, Basis Expansion, Logistic Regression, Discriminant  Analysis, Boosting, Neural Nets...**\n",
    "\n",
    "4. **Run on test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import math\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "#BaseEstimator will inherit get_parms and set_parms methods. \n",
    "#TransformerMixin will inherit fit_transform, which calls fit and transform. We can customize our fit and transform\n",
    "#These are used for consistency with existing sklearn classes\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_pickle(path):\n",
    "    \n",
    "    input_file = open(path,'rb')\n",
    "    variable = pickle.load(input_file)\n",
    "    input_file.close()\n",
    "    return(variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = '../data/x_train.pickle'\n",
    "x_test = '../data/x_test.pickle'\n",
    "y_train = '../data/y_train.pickle'\n",
    "y_test = '../data/y_test.pickle'\n",
    "\n",
    "x_train = read_pickle(x_train) \n",
    "x_test = read_pickle(x_test) \n",
    "y_train = read_pickle(y_train) \n",
    "y_test = read_pickle(y_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations ###\n",
    "\n",
    "Let's begin with applying the transformations we deemed suitable during EDA. \n",
    "1. Standardize the data, \n",
    "2. Remove crit and crit per level variables \n",
    "3. Log cs feature \n",
    "4. Create per level * gamelength variable\n",
    "5. Perform PCA with 30 components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, remove_features):\n",
    "        self.remove_features = remove_features\n",
    "    \n",
    "    def fit(self, x_df):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x_df):\n",
    "        keep_features = list(x_df.columns)\n",
    "        \n",
    "        for feature in self.remove_features:\n",
    "            keep_features.remove(feature)\n",
    "        \n",
    "        x_df = x_df[keep_features]\n",
    "        return(x_df)\n",
    "    \n",
    "class FeatureEngineering(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    #Initiate class\n",
    "    def __init__(self): \n",
    "        return None\n",
    "        \n",
    "    #We don't need to fit anything, so leave this as is\n",
    "    def fit(self, x_df):\n",
    "        return self\n",
    "    \n",
    "    #Perform our feature transformations\n",
    "    def transform(self, x_df):\n",
    "        \n",
    "        #Log cs field\n",
    "        add_constant = abs(min(x_df['delta_total_cs']))\n",
    "        x_df['log_delta_total_cs'] = x_df['delta_total_cs'].apply(lambda x : math.log(x + add_constant + 0.01))\n",
    "        x_df = x_df.drop('delta_total_cs', axis = 1)\n",
    "        \n",
    "        #Create per_level * gamelength variables\n",
    "        feature_columns = list(x_df.columns)\n",
    "        per_level = [feature for feature in feature_columns if \"perlevel\" in feature]\n",
    "        \n",
    "        for i in per_level:\n",
    "            field_name = i + str('_gamelength')\n",
    "            x_df[field_name] = x_df[i] * x_df['gamelength']\n",
    "        \n",
    "        #Standardize data\n",
    "        standard_scaler = preprocessing.StandardScaler()\n",
    "        x_df = standard_scaler.fit_transform(x_df)\n",
    "        \n",
    "        return(x_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    \n",
    "    ('FeatureSelector', FeatureSelector(['delta_crit', 'delta_critperlevel', 'gameid'])),\n",
    "    ('FeatureEngineering', FeatureEngineering()),\n",
    "    ('PCA', PCA(n_components = 30))\n",
    "])\n",
    "\n",
    "x_train_prepared  = pipeline.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training ###\n",
    "\n",
    "List of initial candiate models:\n",
    "\n",
    "1. Logistic Regression\n",
    "2. CART models\n",
    "3. Bagging (Random Forest)\n",
    "4. Boosting\n",
    "5. Others..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Plot ROC Curve\n",
    "def plot_roc_curve(fpr, tpr, label = None):\n",
    "    plt.plot(fpr, tpr, linewidth = 2, label = label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin with zero tuning, just to grasp a baseline for each algorithm. Then we'll start tuning and compare the best model of each algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.41803981 0.38161639 0.44280744 0.39605902 0.50487816 0.42008403\n",
      " 0.45374261 0.43159531 0.47720307 0.4975186 ]\n",
      "0.44235444283898906\n"
     ]
    }
   ],
   "source": [
    "#Begin with baseline logistic regression\n",
    "log_reg_model = LogisticRegression()\n",
    "log_reg_model.fit(x_train_prepared, y_train)\n",
    "\n",
    "scores = cross_val_score(log_reg_model, x_train_prepared, y_train,\n",
    "                        scoring = \"neg_mean_squared_error\", cv = 10)\n",
    "rmse_scores = np.sqrt(-scores)\n",
    "print(rmse_scores)\n",
    "print(np.mean(rmse_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.52138725 0.59119757 0.57735027 0.48507125 0.60228324 0.56879646\n",
      " 0.57735027 0.56011203 0.55401326 0.60525749]\n",
      "0.564281908567188\n"
     ]
    }
   ],
   "source": [
    "#CART Model\n",
    "cart_model = DecisionTreeClassifier()\n",
    "cart_model.fit(x_train_prepared, y_train)\n",
    "\n",
    "scores = cross_val_score(cart_model, x_train_prepared, y_train,\n",
    "                        scoring = \"neg_mean_squared_error\", cv = 10)\n",
    "rmse_scores = np.sqrt(-scores)\n",
    "print(rmse_scores)\n",
    "print(np.mean(rmse_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49266464 0.49266464 0.47485808 0.46442036 0.55129082 0.54232614\n",
      " 0.50487816 0.51449576 0.56287804 0.5073714 ]\n",
      "0.5107848047502787\n"
     ]
    }
   ],
   "source": [
    "#Random Forest Model\n",
    "rand_forest_model = RandomForestClassifier()\n",
    "rand_forest_model.fit(x_train_prepared, y_train)\n",
    "\n",
    "scores = cross_val_score(rand_forest_model, x_train_prepared, y_train,\n",
    "                        scoring = \"neg_mean_squared_error\", cv = 10)\n",
    "rmse_scores = np.sqrt(-scores)\n",
    "print(rmse_scores)\n",
    "print(np.mean(rmse_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.41803981 0.44065265 0.44280744 0.42008403 0.48507125 0.48507125\n",
      " 0.49507377 0.48507125 0.52652419 0.54500431]\n",
      "0.47433999522447784\n"
     ]
    }
   ],
   "source": [
    "#Boosting Model\n",
    "gboost_model = GradientBoostingClassifier()\n",
    "gboost_model.fit(x_train_prepared, y_train)\n",
    "\n",
    "scores = cross_val_score(gboost_model, x_train_prepared, y_train,\n",
    "                        scoring = \"neg_mean_squared_error\", cv = 10)\n",
    "rmse_scores = np.sqrt(-scores)\n",
    "print(rmse_scores)\n",
    "print(np.mean(rmse_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Simple Classification Tree ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_scores = cross_val_predict(log_reg_model, x_train_prepared, y_train, cv = 10, method = 'decision_function')\n",
    "#fpr, tpr, thresholds = roc_curve(y_train, y_scores)\n",
    "#plot_roc_curve(fpr, tpr)\n",
    "#plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
