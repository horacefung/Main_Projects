{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa as tsa\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pickle(path):\n",
    "    \n",
    "    input_file = open(path,'rb')\n",
    "    variable = pickle.load(input_file)\n",
    "    input_file.close()\n",
    "    return(variable)\n",
    "\n",
    "def downsample(df, downsample_step = 10):\n",
    "    \n",
    "    # Downsample\n",
    "    df['downsample'] = df['time_period'] % downsample_step\n",
    "    df = df[df['downsample'] == 0]\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "484378"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in data\n",
    "data = read_pickle('../data/watch/processed_data/train_df.pkl')\n",
    "\n",
    "# Downsample, we don't need that many observations\n",
    "data = downsample(data, downsample_step = 5)\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.drop('is_exercise', axis = 1)\n",
    "y = data['is_exercise']\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot ROC Curve\n",
    "def plot_roc_curve(fpr, tpr, label = None):\n",
    "    plt.plot(fpr, tpr, linewidth = 2, label = label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    \n",
    "#Plot basic diagnostics\n",
    "def classification_diagnostics(model, x, y, standardize = True, classifier = True):\n",
    "    \n",
    "    fig, axs = plt.subplots(nrows = 1, ncols=2, figsize = (15,4))\n",
    "    \n",
    "    if classifier == True:\n",
    "        \n",
    "        y_hat = model.predict_proba(x)[:, 1]\n",
    "        residuals = y - y_hat\n",
    "        std_error = math.sqrt(np.var(residuals))\n",
    "        std_residuals = residuals/std_error\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        y_hat = model.predict(x)\n",
    "        residuals = y_hat - y\n",
    "        std_error = math.sqrt(np.var(residuals))\n",
    "        std_residuals = residuals/std_error\n",
    "        \n",
    "    #Residual vs Fitted Plot\n",
    "    sns.regplot(y_hat, \n",
    "                std_residuals, \n",
    "                lowess=True,\n",
    "                line_kws={'color': 'red', 'lw': 1, 'alpha': 1},\n",
    "                ax = axs[0])\n",
    "    \n",
    "    #Histogram of residuals\n",
    "    sns.distplot(std_residuals, \n",
    "                hist=True,\n",
    "                ax = axs[1])\n",
    "    \n",
    "    #Labels\n",
    "    axs[0].set(xlabel=\"Fitted Values\", \n",
    "                ylabel=\"Pearson's Standardized Residuals\", \n",
    "                title = 'Residual vs Fitted')\n",
    "    \n",
    "    axs[1].set(xlabel=\"Fitted Values\", \n",
    "                ylabel=\"Frequency\", \n",
    "                title = \"Residuals Histogram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, select_features):\n",
    "        self.select_features = select_features\n",
    "    \n",
    "    def fit(self, x_df, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x_df, y=None):\n",
    "        select_features = self.select_features\n",
    "        x_df = x_df[select_features]\n",
    "        return(x_df)\n",
    "\n",
    "    \n",
    "class ContinuousFeatureEngineering(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    # Initiate class\n",
    "    def __init__(self, window):\n",
    "        self.window = window\n",
    "        return None\n",
    "        \n",
    "    # We don't need to fit anything, so leave this as is\n",
    "    def fit(self, df, y=None):\n",
    "        return self\n",
    "    \n",
    "    # Perform our feature transformations\n",
    "    def transform(self, df, y=None):\n",
    "        \n",
    "        x_df = df.copy(deep = True)\n",
    "        \n",
    "        col_names = x_df.columns\n",
    "        \n",
    "        # Time-Series transformations\n",
    "        for col in col_names:\n",
    "            \n",
    "            # Box-Cox estimation\n",
    "            x_df[col] = x_df[col] - min(x_df[col]) + 0.0001\n",
    "            y, fitted_lambda = scipy.stats.boxcox(x_df[col],lmbda = None)\n",
    "        \n",
    "            # First difference, make first element nan, loss from differencing\n",
    "            y = np.append([np.nan], np.diff(y, n=1))\n",
    "            x_df[col] = y\n",
    "        \n",
    "        # Add volatility columns\n",
    "        for col in col_names:\n",
    "            x_df[col + '_vol'] = x_df[col].rolling(self.window).std()\n",
    "        \n",
    "        # Fill nan values with mean\n",
    "        x_df = x_df.fillna(x_df.mean())\n",
    "        \n",
    "        # Standardize data\n",
    "        standard_scaler = preprocessing.StandardScaler()\n",
    "        x_df = standard_scaler.fit_transform(x_df)\n",
    "        \n",
    "        return x_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----Feature Pipeline ----#\n",
    "num_attributes = ['accel_x', 'accel_y', 'accel_z', 'gyro_x', 'gyro_y', 'gyro_z']\n",
    "window = 50\n",
    "\n",
    "numerical_pipeline = Pipeline([\n",
    "\n",
    "    ('FeatureSelector', FeatureSelector(num_attributes)),\n",
    "    ('FeatureEngineering', ContinuousFeatureEngineering(window))\n",
    "])\n",
    "\n",
    "feature_pipeline = FeatureUnion([\n",
    "        ('numerical_pipeline', numerical_pipeline)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test some algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_x_train = feature_pipeline.fit_transform(x_train)\n",
    "proccessed_x_test = feature_pipeline.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV AUC Scores:  [0.66273677 0.65503826 0.65611071 0.6612758  0.66029784 0.66107315\n",
      " 0.66935034 0.66424128 0.665273   0.66616448]\n",
      "Mean CV AUC Scores:  0.6621561634993807\n"
     ]
    }
   ],
   "source": [
    "log_model = LogisticRegression()\n",
    "log_model.fit(processed_x_train, y_train)\n",
    "scores = cross_val_score(log_model, processed_x_train, y_train,\n",
    "                        scoring = \"roc_auc\", cv = 10)\n",
    "print('CV AUC Scores: ', scores)\n",
    "print('Mean CV AUC Scores: ', np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CART Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV AUC Scores:  [0.78643484 0.78126546 0.78251569 0.7876979  0.78648604 0.7872575\n",
      " 0.78637583 0.7873258  0.78311762 0.78271116]\n",
      "Mean CV AUC Scores:  0.7851187819993154\n"
     ]
    }
   ],
   "source": [
    "cart_model = DecisionTreeClassifier(min_samples_leaf = 30) #blanket default, to avoid 1 observation per leaf\n",
    "cart_model.fit(processed_x_train, y_train)\n",
    "\n",
    "scores = cross_val_score(cart_model, processed_x_train, y_train,\n",
    "                        scoring = \"roc_auc\", cv = 10)\n",
    "print('CV AUC Scores: ', scores)\n",
    "print('Mean CV AUC Scores: ', np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
