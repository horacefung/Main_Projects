{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### League of Legends: Model Tuning\n",
    "Standard Methodology:\n",
    "\n",
    "1. Exploratory plots to get a sense of data (e.g. relationships, distribution etc.)\n",
    "2. Perform transformations (standardization, log-transform, PCA etc.)\n",
    "3. Experiment with algorithms that make sense, feature selection and compare cross-validated performance.Algos to thinks about: Tree-Based, Basis Expansion, Logistic Regression, Discriminant  Analysis, Boosting, Neural Nets...\n",
    "\n",
    "4. **Tune model hyperparameters**\n",
    "5. **Run on test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import math\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import scipy.stats as stats\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_pickle(path):\n",
    "    \n",
    "    input_file = open(path,'rb')\n",
    "    variable = pickle.load(input_file)\n",
    "    input_file.close()\n",
    "    return(variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train = '../data/x_train_v2.pickle'\n",
    "x_test = '../data/x_test_v2.pickle'\n",
    "y_train = '../data/y_train_v2.pickle'\n",
    "y_test = '../data/y_test_v2.pickle'\n",
    "\n",
    "x_train = read_pickle(x_train) \n",
    "x_test = read_pickle(x_test) \n",
    "y_train = read_pickle(y_train) \n",
    "y_test = read_pickle(y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeatureSelector(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, select_features):\n",
    "        self.select_features = select_features\n",
    "    \n",
    "    def fit(self, x_df):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x_df):\n",
    "        select_features = self.select_features\n",
    "        x_df = x_df[select_features]\n",
    "        return(x_df)\n",
    "    \n",
    "class ContinuousFeatureEngineering(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    #Initiate class\n",
    "    def __init__(self): \n",
    "        return None\n",
    "        \n",
    "    #We don't need to fit anything, so leave this as is\n",
    "    def fit(self, x_df):\n",
    "        return self\n",
    "    \n",
    "    #Perform our feature transformations\n",
    "    def transform(self, x_df):\n",
    "        \n",
    "        #Log cs field\n",
    "        add_constant = abs(min(x_df['delta_total_cs']))\n",
    "        x_df['log_delta_total_cs'] = x_df['delta_total_cs'].apply(lambda x : math.log(x + add_constant + 0.01))\n",
    "        x_df = x_df.drop('delta_total_cs', axis = 1)\n",
    "        \n",
    "        #Create per_level * gamelength variables\n",
    "        feature_columns = list(x_df.columns)\n",
    "        per_level = [feature for feature in feature_columns if \"perlevel\" in feature]\n",
    "        \n",
    "        for i in per_level:\n",
    "            field_name = i + str('_average_gamelength')\n",
    "            x_df[field_name] = x_df[i] * x_df['average_gamelength']\n",
    "        \n",
    "        #Standardize data\n",
    "        standard_scaler = preprocessing.StandardScaler()\n",
    "        x_df = standard_scaler.fit_transform(x_df)\n",
    "        \n",
    "        return(x_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_attributes = ['delta_assists', 'delta_damagetochampions', 'delta_deaths',\n",
    "       'delta_kills', 'delta_monsterkills', 'delta_total_cs',\n",
    "       'delta_totalgold', 'delta_wardskilled', 'delta_wardsplaced',\n",
    "       'delta_armor', 'delta_armorperlevel', 'delta_attackdamage',\n",
    "       'delta_attackdamageperlevel', 'delta_attackrange', 'delta_attackspeed',\n",
    "       'delta_attackspeedperlevel','delta_gap_closer_value', 'delta_hard_cc_value', 'delta_hp',\n",
    "       'delta_hpperlevel', 'delta_hpregen', 'delta_hpregenperlevel',\n",
    "       'delta_movespeed', 'delta_mp', 'delta_mpperlevel', 'delta_mpregen',\n",
    "       'delta_mpregenperlevel', 'delta_protection_value',\n",
    "       'delta_soft_cc_value', 'delta_spellblock', 'delta_spellblockperlevel',\n",
    "       'delta_spells_average_range_value', 'delta_Assassin', 'delta_Fighter',\n",
    "       'delta_Mage', 'delta_Marksman', 'delta_Support', 'delta_Tank',\n",
    "       'average_gamelength']\n",
    "\n",
    "categorical_attributes = ['soul_point', 'red_soul_point']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "numerical_pipeline = Pipeline([\n",
    "    \n",
    "    ('FeatureSelector', FeatureSelector(num_attributes)),\n",
    "    ('FeatureEngineering', ContinuousFeatureEngineering()),\n",
    "    ('PCA', PCA(n_components = 30))\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline([\n",
    "    \n",
    "    ('FeatureSelector', FeatureSelector(num_attributes))\n",
    "])\n",
    "\n",
    "full_pipeline = FeatureUnion(transformer_list = [\n",
    "    ('numerical_pipeline', numerical_pipeline),\n",
    "    ('categorical_pipeline', categorical_pipeline),\n",
    "])\n",
    "\n",
    "x_train_prepared  = full_pipeline.fit_transform(x_train)\n",
    "x_test_prepared = full_pipeline.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Tuning Range\n",
    "\n",
    "Before performing grid-search, which computationally expensive especially if the range & number of hyperparameters are big, we can start by first plotting the fitting curves of each algorithm with respect to their complexity controls. We will compare the training score vs cross validated training scores.\n",
    "\n",
    "1. Logistic Regresion: L1/L2 regularization\n",
    "2. CART: max_depth, min_samples_split, min_samples_leaf\n",
    "3. Random Forest: max_depth, min_samples_split, min_samples_leaf \n",
    "4. Gradient Boosting: max_depth, min_samples_split, min_samples_leaf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cart_model = DecisionTreeClassifier(min_samples_leaf = 30) #blanket default, to avoid 1 observation per leaf\n",
    "cart_model.fit(x_train_prepared, y_train)\n",
    "\n",
    "cv_score = np.mean(cross_val_score(cart_model, x_train_prepared, y_train,scoring = \"roc_auc\", cv = 3))\n",
    "train_score = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auc_train = []\n",
    "auc_test = []\n",
    "maxdepth = 20\n",
    "depths = range(1, maxdepth+1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "for md in depths:\n",
    "    model = DecisionTreeClassifier(max_depth=md)\n",
    "    model.fit(X_train, Y_train)\n",
    "    \n",
    "    # Get the probability of Y_test records being = 1\n",
    "    Y_train_probability_1 = model.predict_proba(X_train)[:, 1]\n",
    "    Y_test_probability_1 = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    # Use the metrics.roc_curve function to get the true positive rate (tpr) and false positive rate (fpr)\n",
    "    fpr_train, tpr_train, thresholds_train = metrics.roc_curve(Y_train, Y_train_probability_1)\n",
    "    fpr_test, tpr_test, thresholds_test = metrics.roc_curve(Y_test, Y_test_probability_1)\n",
    "    \n",
    "    auc_train.append(metrics.auc(fpr_train, tpr_train))\n",
    "    auc_test.append(metrics.auc(fpr_test, tpr_test))\n",
    "    \n",
    "plt.plot(depths, auc_train, label=\"Train\")\n",
    "plt.plot(depths, auc_test, label=\"Test\")\n",
    "plt.title(\"Decision Tree AUC Performance on train and test data\")\n",
    "plt.xlabel(\"Max depth\")\n",
    "plt.ylabel(\"AUC\")\n",
    "plt.ylim([min(auc_test), 1.0])\n",
    "plt.xlim([1,maxdepth])\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
